<!DOCTYPE html>

<html lang="en">

<head>
    
    <title>Hexo</title>
    <meta charset="UTF-8">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
    
    

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <meta name="description" content="Hadoop 概述 尚硅谷Hadoop教程：BV1cW411r7c5  1. 大数据概论 大数据主要解决海量数据的存储和分析计算问题 海量指 TB PB EB 级别数据 大数据特点： Volume（大量）、Velocity（高速）、Variety（多样）、Value（低价值密度） 数据分为结构化数据（数据库&#x2F;文本为主）和非结构化数据（网络日志、音视频、图片、地理位置信息等）  2. Hadoop">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/Notes/HadoopNote_01.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Hadoop 概述 尚硅谷Hadoop教程：BV1cW411r7c5  1. 大数据概论 大数据主要解决海量数据的存储和分析计算问题 海量指 TB PB EB 级别数据 大数据特点： Volume（大量）、Velocity（高速）、Variety（多样）、Value（低价值密度） 数据分为结构化数据（数据库&#x2F;文本为主）和非结构化数据（网络日志、音视频、图片、地理位置信息等）  2. Hadoop">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-06-19T08:57:39.238Z">
<meta property="article:modified_time" content="2021-06-19T07:36:06.825Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/npm/highlight.js@9.15.8/styles/atom-one-dark.css,npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css,gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css,npm/hexo-theme-nexmoe@latest/source/lib/mdui_043tiny/css/mdui.css,npm/hexo-theme-nexmoe@latest/source/lib/iconfont/iconfont.css?v=233" crossorigin>
    <link rel="stylesheet" href="/css/style.css?v=1630854088403">
     
    
    <link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1630854088403">
    
        <link rel="stylesheet" href="/custom.css">
    
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="mdui-drawer-body-left">
    
    <div id="nexmoe-background">
        <div class="nexmoe-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/cover/5c3aec85a4343.jpg)"></div>
        <div class="mdui-appbar mdui-shadow-0">
            <div class="mdui-toolbar">
                <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
                <div class="mdui-toolbar-spacer"></div>
                <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
                <a href="/" title="John Doe" class="mdui-btn mdui-btn-icon"><img src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/avatar.png" alt="John Doe"></a>
            </div>
        </div>
    </div>
    <div id="nexmoe-header">
        <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="John Doe">
            <img src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/avatar.png" alt="John Doe" alt="John Doe">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>Articles</span>1</div>
        <div><span>Tags</span>0</div>
        <div><span>Categories</span>0</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/about.html" title="关于博客">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博客
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/PY.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
    
    <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
         
            <form id="search_form" action_e="https://cn.bing.com/search?q=site:nexmoe.com" onsubmit="return search();">
                <label><input id="search_value" name="q" type="search" placeholder="Search"></label>
            </form>
         
    </div>
</div>
    
    <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="https://jq.qq.com/?_wv=1027&k=5CfKHun" target="_blank" mdui-tooltip="{content: 'QQ群'}" style="color: rgb(249, 174, 8);background-color: rgba(249, 174, 8, .1);">
            <i class="nexmoefont icon-QQ"></i>
        </a><a class="mdui-ripple" href="https://space.bilibili.com/20238211" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/nexmoe/" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a>
    </div>
</div>
    
    

    
    
    
</aside>
    <div class="nexmoe-copyright">
        &copy; 2021 John Doe
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
        <br><a target="_blank" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"><img src="https://i.dawnlab.me/c0268c1e6cfd0863d6ba35be1575941a.png" width="150px"></a><script data-ad-client="ca-pub-2058306854838448" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    </div>
</div><!-- .nexmoe-drawer -->
    </div>
    <div id="nexmoe-content">
        <div class="nexmoe-primary">
            <div class="nexmoe-post">

  <article>
      
          <div class="nexmoe-post-cover" style="padding-bottom: 66.66666666666666%;"> 
              <img data-src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/cover/5c3aec85a4343.jpg" data-sizes="auto" alt="" class="lazyload">
              <h1></h1>
          </div>
      
      
      <div class="nexmoe-post-meta nexmoe-rainbow" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2021年06月19日</a>
    <a><i class="nexmoefont icon-areachart"></i>6.1k 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 29 分钟</a>
</div>

      

      <h1 id="Hadoop-概述"><a href="#Hadoop-概述" class="headerlink" title="Hadoop 概述"></a>Hadoop 概述</h1><blockquote>
<p>尚硅谷Hadoop教程：BV1cW411r7c5</p>
</blockquote>
<h2 id="1-大数据概论"><a href="#1-大数据概论" class="headerlink" title="1. 大数据概论"></a>1. 大数据概论</h2><ul>
<li>大数据主要解决海量数据的存储和分析计算问题</li>
<li>海量指 <code>TB</code> <code>PB</code> <code>EB</code> 级别数据</li>
<li>大数据特点： Volume（大量）、Velocity（高速）、Variety（多样）、Value（低价值密度）</li>
<li>数据分为结构化数据（数据库/文本为主）和非结构化数据（网络日志、音视频、图片、地理位置信息等）</li>
</ul>
<h2 id="2-Hadoop-基础"><a href="#2-Hadoop-基础" class="headerlink" title="2. Hadoop 基础"></a>2. Hadoop 基础</h2><h3 id="2-1-Hadoop-介绍"><a href="#2-1-Hadoop-介绍" class="headerlink" title="2.1 Hadoop 介绍"></a>2.1 Hadoop 介绍</h3><ul>
<li>Hadoop 是由 Apache 基金会开发的分布式系统基础架构</li>
<li>Hadoop 主要解决海量数据的存储和分析计算问题</li>
<li>Hadoop 三个主要发行版本： Apache、Cloudera、Hortonworks</li>
<li>Hadoop 具有高可靠性、高扩展性、高效性和高容错性<ul>
<li>高可靠性：底层维护多个（至少 3 个）数据副本，即使某个计算元素或储存出现故障也不会导致数据丢失</li>
<li>高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点</li>
<li>高效性：在 MapReduce 思想下，Hadoop 是并行工作的，以加快任务处理速度</li>
<li>高容错性：能够自动将失败的任务重新分配</li>
</ul>
</li>
</ul>
<h3 id="2-2-Hadoop-组成"><a href="#2-2-Hadoop-组成" class="headerlink" title="2.2 Hadoop 组成"></a>2.2 Hadoop 组成</h3><ul>
<li>Hadoop 1.x<ul>
<li>MapReduce （计算+资源调度）</li>
<li>HDFS （数据储存）</li>
<li>Common （辅助工具）</li>
</ul>
</li>
<li>Hadoop 2.x<ul>
<li>MapReduce （计算）</li>
<li>Yarn （资源调度）</li>
<li>HDFS （数据储存）</li>
<li>Common （辅助工具）</li>
</ul>
</li>
<li>HDFS 架构<ul>
<li>NameNode (nn)：存储文件的元数据，如文件名、文件目录结构、文件属性（生成时间、副本数、文件权限）以及每个文件的块列表和块所在的 DataNode 等</li>
<li>DataNode (dn)：在本地文件系统储存文件块数据，以及块数据的校验和</li>
<li>Secondary NameNode (2nn)：用来监视 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照</li>
</ul>
</li>
<li>YARN 架构<ul>
<li>ResourceManager (RM)：<ul>
<li>处理客户端请求</li>
<li>监控 NodeManager</li>
<li>启动或监控 ApplicationMaster (MapReduce Status)</li>
</ul>
</li>
<li>NodeManager (NM)：<ul>
<li>管理单个节点上的资源</li>
<li>处理来自 ResourceManager 的命令</li>
<li>处理来自 ApplicationMaster 的命令 </li>
</ul>
</li>
<li>ApplicationMaster (AM)：<ul>
<li>负责数据的切分</li>
<li>为应用程序申请资源并分配给内部的任务</li>
<li>任务的监控与容错</li>
</ul>
</li>
<li>Container<ul>
<li>YARN 中资源的抽象，封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等</li>
</ul>
</li>
</ul>
</li>
<li>MapReduce 架构<ul>
<li>Map 阶段并行处理输入数据</li>
<li>Reduce 阶段对 Map 结果进行汇总</li>
</ul>
</li>
</ul>
<h3 id="2-3-Hadoop-环境搭建"><a href="#2-3-Hadoop-环境搭建" class="headerlink" title="2.3 Hadoop 环境搭建"></a>2.3 Hadoop 环境搭建</h3><ul>
<li><p>VMWare 15 + CentOS 7</p>
<blockquote>
<p>下列 <code>bash</code> 操作如无特殊说明，均是在 <code>root</code> 账户 <code>~</code> 目录下完成</p>
<p>参考了<a target="_blank" rel="noopener" href="https://blog.csdn.net/ErnestW/article/details/88929968?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight">这篇</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/aic1999/article/details/104660016#9-hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEhdfs">这篇</a>文章</p>
</blockquote>
<ul>
<li><p>安装过程略，建议使用<a target="_blank" rel="noopener" href="https://mirrors.bfsu.edu.cn/centos/7.8.2003/isos/x86_64/CentOS-7-x86_64-Minimal-2003.iso">这个镜像</a></p>
</li>
<li><p>在 VMWare 中加载镜像新建虚拟机，建议配置：内存 8G，虚拟硬盘 20G 以上</p>
</li>
<li><p>安装后，设置网络连接</p>
<blockquote>
<p>视实际情况，内容可能有所不同。IP 可在主机上查看 VMnet8 的信息</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line"><span class="comment"># 在文件中添加下列行</span></span><br><span class="line">&gt; IPADDR=192.168.38.129</span><br><span class="line">&gt; GATEWAY=192.168.38.2</span><br><span class="line">&gt; NETMASK=255.255.255.0</span><br><span class="line">&gt; DNS1=114.114.114.114</span><br><span class="line"><span class="comment"># 在文件中修改下列行</span></span><br><span class="line">&gt; BOOTPROTO=none</span><br><span class="line">&gt; ONBOOT=yes</span><br><span class="line">$ service network restart</span><br><span class="line">Restarting network (via systemctl):				[ OK ]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>网络连接设置完成后，换源</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br><span class="line">$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"><span class="comment"># 如果没有 wget 则运行下面这条指令</span></span><br><span class="line"><span class="comment"># $ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">$ yum clean all</span><br><span class="line">$ yum makecache</span><br><span class="line">$ yum list | grep epel-release</span><br><span class="line">$ yum install epel-release</span><br><span class="line"><span class="comment"># 如果没有 wget 则先运行下面这条指令</span></span><br><span class="line"><span class="comment"># $ yum install wget</span></span><br><span class="line">$ wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line">$ yum clean all</span><br><span class="line">$ yum makecache</span><br></pre></td></tr></table></figure></li>
<li><p>更改 Hosts</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/hosts</span><br><span class="line"><span class="comment"># 在文件中添加如下行</span></span><br><span class="line">&gt; 192.168.38.129 Master</span><br><span class="line">&gt; 192.168.38.130 Slave1</span><br><span class="line">&gt; 192.168.38.131 Slave2</span><br><span class="line">$ vi /etc/hostname</span><br><span class="line"><span class="comment"># 在文件中修改，对两台 Slave 从机做同样操作</span></span><br><span class="line">&gt; Master</span><br></pre></td></tr></table></figure></li>
<li><p>安装 Jdk-1.8 和 Hadoop </p>
<blockquote>
<p>Oracle 账号密码来自<a target="_blank" rel="noopener" href="https://blog.csdn.net/u010590120/article/details/94736800?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight">这篇文章</a>，由于直接下载会存在无法解压的问题，所以在 Windows 本地下载再通过 scp 上传到虚拟机 </p>
<p><del>(傻逼 Oracle) </del></p>
</blockquote>
  <figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">PS</span> E:\MSN\Downloads&gt; scp .\jdk<span class="literal">-8u251</span><span class="literal">-linux</span><span class="literal">-x64</span>.tar.gz root@<span class="number">192.168</span>.<span class="number">38.129</span>:/root/jdk<span class="literal">-8u251</span><span class="literal">-linux</span><span class="literal">-x64</span>.tar.gz</span><br><span class="line">root@<span class="number">192.168</span>.<span class="number">38.129</span><span class="string">&#x27;s password:</span></span><br><span class="line"><span class="string">jdk-8u251-linux-x64.tar.gz                                                            100%  186MB 114.9MB/s   00:01</span></span><br></pre></td></tr></table></figure>

  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf jdk-8u251-linux-x64.tar.gz -C /usr/<span class="built_in">local</span>/java</span><br><span class="line">$ vim /etc/profile</span><br><span class="line"><span class="comment"># 在文件中添加如下行</span></span><br><span class="line">&gt; <span class="built_in">export</span> JAVA_HOME=<span class="string">&quot;/usr/local/java/jdk1.8.0_251&quot;</span></span><br><span class="line">&gt; <span class="built_in">export</span> JRE_HOME=<span class="string">&quot;<span class="variable">$JAVA_HOME</span>/jre&quot;</span></span><br><span class="line">&gt; <span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$JRE_HOME</span>/bin&quot;</span></span><br><span class="line">&gt; <span class="built_in">export</span> CLASSPATH=<span class="string">&quot;.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JRE_HOME</span>/lib&quot;</span></span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">$ wget https://mirrors.aliyun.com/apache/hadoop/core/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line">$ tar -zxvf hadoop-3.2.1.tar.gz -C /usr/<span class="built_in">local</span>/hadoop</span><br></pre></td></tr></table></figure></li>
<li><p>配置密钥</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">&#x27;&#x27;</span></span><br><span class="line">  $ <span class="built_in">cd</span> /root/.ssh</span><br><span class="line">  [.ssh] $ touch /root/.ssh/authorized_keys</span><br><span class="line">  [.ssh] $ vim authorized_keys</span><br><span class="line">  <span class="comment"># 将三台虚拟机的 id_rsa.pub 全部保存至 authorized_keys 中，之后用 ssh 测试链接</span></span><br><span class="line">  [.ssh] $ ssh Master <span class="comment"># 或 Slave1 Slave2</span></span><br></pre></td></tr></table></figure></li>
<li><p>配置 Hadoop</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /root/hadoop</span><br><span class="line">  $ mkdir /root/hadoop/tmp</span><br><span class="line">  $ mkdir /root/hadoop/var</span><br><span class="line">  $ mkdir /root/hadoop/dfs</span><br><span class="line">  $ mkdir /root/hadoop/dfs/name</span><br><span class="line">  $ mkdir /root/hadoop/dfs/data</span><br><span class="line">  <span class="comment"># 修改 etc/hadoop 下的配置文件</span></span><br><span class="line">  $ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop/hadoop-3.2.1/etc/hadoop</span><br><span class="line">  [hadoop] $ vim hadoop-env.sh</span><br><span class="line">  <span class="comment"># 添加 JAVA_HOME</span></span><br><span class="line">  &gt; <span class="built_in">export</span> JAVA_HOME=<span class="string">&quot;/usr/local/java/jdk1.8.0_251&quot;</span></span><br><span class="line">  [hadoop] $ vim core-site.xml</span><br><span class="line">  <span class="comment"># 添加如下配置</span></span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;/root/hadoop/tmp&lt;/value&gt;</span><br><span class="line">  &gt;        &lt;description&gt;Abase <span class="keyword">for</span> other temporary directories.&lt;/description&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;hdfs://Master:9000&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  [hadoop] $ vim hdfs-site.xml</span><br><span class="line">  <span class="comment"># 添加如下配置</span></span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;       &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;/root/hadoop/dfs/name&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;description&gt;Path on the <span class="built_in">local</span> filesystem <span class="built_in">where</span> theNameNode stores the namespace and transactions logs persistently.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;       &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;/root/hadoop/dfs/data&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;description&gt;Comma separated list of paths on the localfilesystem of a DataNode <span class="built_in">where</span> it should store its blocks.&lt;/description&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;       &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;       &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">  &gt;       &lt;description&gt;need not permissions&lt;/description&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  [hadoop] $ vim mapred-site.xml</span><br><span class="line">  <span class="comment"># 添加如下配置</span></span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;   &lt;name&gt;mapred.job.tracker&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;Master:49001&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;       &lt;name&gt;mapred.local.dir&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;/root/hadoop/var&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;       &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &gt;       &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  [hadoop] $ vim yarn-site.xml</span><br><span class="line">  <span class="comment"># 添加如下配置</span></span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;Master&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;description&gt;The address of the applications manager interface <span class="keyword">in</span> the RM.&lt;/description&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="variable">$&#123;yarn.resourcemanager.hostname&#125;</span>:8032&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;description&gt;The address of the scheduler interface.&lt;/description&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="variable">$&#123;yarn.resourcemanager.hostname&#125;</span>:8030&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;description&gt;The http address of the RM web application.&lt;/description&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="variable">$&#123;yarn.resourcemanager.hostname&#125;</span>:8088&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;description&gt;The https adddress of the RM web application.&lt;/description&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.webapp.https.address&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="variable">$&#123;yarn.resourcemanager.hostname&#125;</span>:8090&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="variable">$&#123;yarn.resourcemanager.hostname&#125;</span>:8031&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;description&gt;The address of the RM admin interface.&lt;/description&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="variable">$&#123;yarn.resourcemanager.hostname&#125;</span>:8033&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">  &gt;        &lt;discription&gt;每个节点可用内存,单位MB,默认8192MB&lt;/discription&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;2.1&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  &gt;   &lt;property&gt;</span><br><span class="line">  &gt;        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">  &gt;        &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">  &gt;   &lt;/property&gt;</span><br><span class="line">  [hadoop] $ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop/hadoop-3.2.1/sbin</span><br><span class="line">  [sbin] $ vim start-dfs.sh</span><br><span class="line">  <span class="comment"># 添加以下行</span></span><br><span class="line">  &gt; HDFS_DATANODE_USER=root</span><br><span class="line">  &gt; HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">  &gt; HDFS_NAMENODE_USER=root</span><br><span class="line">  &gt; HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">  [sbin] $ vim stop-dfs.sh</span><br><span class="line">  <span class="comment"># 添加以下行</span></span><br><span class="line">  &gt; HDFS_DATANODE_USER=root</span><br><span class="line">  &gt; HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">  &gt; HDFS_NAMENODE_USER=root</span><br><span class="line">  &gt; HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">  [sbin] $ vim start-yarn.sh</span><br><span class="line">  <span class="comment"># 添加以下行</span></span><br><span class="line">  &gt; YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">  &gt; HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">  &gt; YARN_NODEMANAGER_USER=root</span><br><span class="line">  [sbin] $ vim stop-yarn.sh</span><br><span class="line">  <span class="comment"># 添加以下行</span></span><br><span class="line">  &gt; YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">  &gt; HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">  &gt; YARN_NODEMANAGER_USER=root</span><br><span class="line">  [sbin] $ vim /etc/selinux/config</span><br><span class="line">  <span class="comment"># 修改以下行</span></span><br><span class="line">  &gt; SELINUX=disabled</span><br><span class="line">  [sbin] $ systemctl <span class="built_in">disable</span> firewalld.service</span><br><span class="line">  [sbin] $ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop/hadoop-3.2.1/etc/hadoop</span><br><span class="line">  [hadoop] $ touch Master <span class="comment"># 从机的话文件名是从机名字</span></span><br><span class="line">  [hadoop] $ vim Master</span><br><span class="line">  <span class="comment"># 主机写入主机名，从机写入主机名和自己的名字</span></span><br><span class="line">  &gt; Master</span><br><span class="line">  [hadoop] $ vim workers</span><br><span class="line">  &gt; Master</span><br><span class="line">  &gt; Slave1</span><br><span class="line">  &gt; Slave2</span><br></pre></td></tr></table></figure>

<blockquote>
<p>  如果运行示例时报错</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Please check whether your etc/hadoop/mapred-site.xml contains the below configuration:</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;full path of your hadoop distribution directory&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;full path of your hadoop distribution directory&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;full path of your hadoop distribution directory&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>  则把 <code>$&#123;full path of your hadoop distribution directory&#125;</code> 改成安装 hadoop 的位置</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop/hadoop-3.2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br></pre></td></tr></table></figure>
</blockquote>
<ul>
<li><p>启动 Hadoop</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop/hadoop-3.2.1/bin</span><br><span class="line">  [bin] $ ./hadoop namenode -format</span><br><span class="line">  [bin] $ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop/hadoop-3.2.1/sbin</span><br><span class="line">  [sbin] $ ./start-all.sh</span><br><span class="line">  <span class="comment"># $ ./stop-all.sh</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Hadoop 运行模式</p>
<ul>
<li><p>本地运行模式</p>
<ul>
<li>参考官方文档，注意每次需要删除 output 文件夹</li>
</ul>
</li>
<li><p>伪分布式运行模式</p>
<ul>
<li>配置同完全分布式，但是只有 Master 机</li>
<li>上传文件 <code>bin/hdfs dfs -put</code></li>
<li>格式化 NameNode 注意事项<ol>
<li>在 <code>dfs/name/current/VERSION</code> 文件里有 <code>clusterID</code></li>
<li>在 <code>dfs/data/current/VERSION</code> 里一样有 <code>clusterID</code> 并且跟上面的一样</li>
<li>重新格式化时，两边的 <code>clusterID</code> 不一样，无法通讯， DataNode 和 NameNode 无法通信</li>
<li>重新格式化之前，需要删除 DataNode 里面的信息</li>
</ol>
</li>
<li>启动 YARN 并运行 MapReduce 程序<ol>
<li>在 <code>yarn-env.sh</code> 里配置 <code>JAVA_HOME</code></li>
<li>在 <code>mapred-env.sh</code> 里配置 <code>JAVA_HOME</code></li>
</ol>
</li>
</ul>
</li>
<li><p>完全分布式运行模式</p>
<ul>
<li>rsync 更新差异文件，主要用于备份和镜像</li>
<li>xsync 集群分发脚本，循环复制文件到所有节点的相同目录下</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span>((pcount==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`basename <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line"></span><br><span class="line">pdir=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> pdir=<span class="variable">$pdir</span></span><br><span class="line"></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>((snum=1; snum&lt;3; snum++)); <span class="keyword">do</span></span><br><span class="line">	<span class="built_in">echo</span> -----Slave<span class="variable">$snum</span>-----</span><br><span class="line">	srync -rvl <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span>@Slave<span class="variable">$snum</span>:<span class="variable">$pdir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<ul>
<li>可以把 NameNode，ResourceManager 和 SecondaryNameNode 放在不同节点，配置 .xml 文件即可</li>
<li>上传数据</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs <span class="variable">$target_dir</span> dsf -put <span class="variable">$source_dir</span></span><br></pre></td></tr></table></figure>

<ul>
<li>上传的数据储存在 <code>data/tmp/dfs/data/current/$ClusterID/current/finalized/subdir</code></li>
<li>YARN 应该在 ResourceManager 所在的机器上启动</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3-HDFS"><a href="#3-HDFS" class="headerlink" title="3. HDFS"></a>3. HDFS</h2><h3 id="3-1-HDFS-概述"><a href="#3-1-HDFS-概述" class="headerlink" title="3.1 HDFS 概述"></a>3.1 HDFS 概述</h3><ul>
<li>HDFS 是分布式文件管理系统，通过<strong>目录树</strong>来定位文件</li>
<li>HDFS 适合一次写入多次读出的场景，且不支持文件的修改，适合用来数据分析，并不适合网盘应用</li>
<li>优点<ul>
<li>高容错性：数据自动保存多个副本，某一个副本丢失后可以自动恢复</li>
<li>适合处理大数据：处理规模最高可达 <code>PB</code> 级别和百万以上规模的文件数量</li>
<li>可构建在廉价机器上</li>
</ul>
</li>
<li>缺点<ul>
<li>不适合低延时的数据访问</li>
<li>无法高效的对大量小文件进行存储：储存大量小文件会占用 NameNode 大量的内存储存文件目录和块信息</li>
<li>不支持并发写入和文件随机修改：一个文件只能由一个写，且只支持数据的追加</li>
</ul>
</li>
<li>HDFS 组成架构<ul>
<li>NameNode：储存了 Metadata。管理 HDFS 的名称空间和 Block 映射信息，配置副本策略和处理客户端读写请求</li>
<li>DataNode：储存 Blocks，执行 Blocks 的读写操作</li>
<li>Client：客户端。进行文件切分上传，与 NameNode 交互获取文件位置信息，与 DataNode 交互读写数据，管理和访问 HDFS</li>
<li>SecondaryNameNode：辅助 NameNode工作，如定期合并 Fsimage 和 Edits 并推送给 NameNode；紧急情况下辅助回复 NameNode。不是 NameNode 的热备份</li>
</ul>
</li>
<li>HDFS 的 Blocks 大小<ul>
<li>通过参数 <code>dfs.blocksize</code> 确定， 2.x 版本默认为 128 MB，1.x 版本默认是 64 MB</li>
<li>寻址时间为传输时间的 1% 时为最佳状态</li>
<li>Blocks 太小，会增加寻址时间；Blocks 太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间（即寻址时间），导致程序处理数据时过慢</li>
</ul>
</li>
</ul>
<h3 id="3-2-HDFS-Shell-命令"><a href="#3-2-HDFS-Shell-命令" class="headerlink" title="3.2 HDFS Shell 命令"></a>3.2 HDFS Shell 命令</h3><blockquote>
<p>基本语法：<code>bin/hadoop fs command</code> 或者 <code>bin/hdfs dfs command</code>。<code>dfs</code> 是 <code>fs</code> 的实现类。</p>
</blockquote>
<ul>
<li>输入 <code>dfs</code> 或 <code>fs</code> 会显示命令参数列表</li>
<li>常用命令<ul>
<li><code>-help</code> 输出命令参数</li>
<li><code>-ls</code> 查看目录信息</li>
<li><code>-mkdir</code> 创建目录</li>
<li><code>-moveFromLocal</code> 从本地上传到 HDFS（剪切）</li>
<li> <code>-appendToFile</code> 追加内容到文件末尾</li>
<li><code>-cat</code> 查看文件内容</li>
<li><code>-chgrp</code> <code>-chown</code> <code>-chmod</code> 修改文件所属权限</li>
<li><code>-copyFromLocal</code> 从本地复制到 HDFS</li>
<li><code>-copyToLocal</code> 从 HDFS 复制到本地</li>
<li><code>-cp</code> 从 HDFS 路径复制到另一个 HDFS 路径</li>
<li> <code>-mv</code> 从 HDFS 路径移动到另一个 HDFS 路径</li>
<li><code>-get </code> 等于 <code>-copyToLocal</code></li>
<li><code>-put</code> 等于 <code>-copyFromLocal</code></li>
<li><code>-getmerge</code> 合并下载多个文件</li>
<li><code>-tail</code> 显示文件末尾</li>
<li><code>-rm</code> 删除文件或文件夹</li>
<li><code>-rmdir</code> 删除空目录</li>
<li><code>-du</code> 统计文件夹大小信息</li>
<li><code>-setrep</code> 设置 HDFS 中文件的副本数量</li>
</ul>
</li>
</ul>
<h3 id="3-3-HDFS-API-操作"><a href="#3-3-HDFS-API-操作" class="headerlink" title="3.3 HDFS API 操作"></a>3.3 HDFS API 操作</h3><ul>
<li><p>在 Java 代码中，通过 FileSystem 提供的与 Shell 命令同名的方法进行操作</p>
</li>
<li><p>参数优先级顺序：客户端代码中的设置 &gt; ClassPath 下用户自定义配置文件 &gt; 服务器默认配置</p>
</li>
<li><p>如果 API 没有提供需要的操作，需要通过 IO 流进行操作</p>
</li>
</ul>
<h3 id="3-4-HDFS-数据流"><a href="#3-4-HDFS-数据流" class="headerlink" title="3.4 HDFS 数据流"></a>3.4 HDFS 数据流</h3><ul>
<li>数据写入流程<ol>
<li>客户端向 NameNode 请求上传文件</li>
<li>NameNode 响应可以上传文件</li>
<li>请求上传第一个 Block，返回 DataNode</li>
<li>NameNode 返回 DataNode 节点</li>
<li>客户端请求与 DataNode 建立通道</li>
<li>DataNode 应答成功</li>
<li>传输数据 Packet</li>
<li>通知 NameNode 传输完成</li>
</ol>
</li>
<li>网络拓扑-节点距离计算<ul>
<li>节点距离：两个节点到达最近的共同祖先的距离总和</li>
</ul>
</li>
<li>机架感知（副本储存节点选择）<ul>
<li>以三个为例。第一个副本位于 Client 所处节点上。如果客户端在集群外，随机选一个</li>
<li>第二个副本和第一个副本位于相同机架上，随机节点</li>
<li>第三个副本位于不同机架随机节点</li>
</ul>
</li>
<li>数据读取流程<ul>
<li>客户端向 NameNode 请求下载文件</li>
<li>NameNode 返回文件的元数据</li>
<li>客户端向 DataNode 请求读取数据</li>
<li>DataNode 传输数据</li>
<li>对所有储存了待下载文件块对 DataNode 重复上述两个操作</li>
</ul>
</li>
</ul>
<h3 id="3-5-HDFS-2-X-新特性"><a href="#3-5-HDFS-2-X-新特性" class="headerlink" title="3.5 HDFS 2.X 新特性"></a>3.5 HDFS 2.X 新特性</h3><ul>
<li>集群间数据拷贝 <code>dictcp</code></li>
<li>小文件存档 HAR 将文件存入 HDFS块，是独立小文件，但是对 NN 而言是一个整体</li>
<li>回收站 <code>fs.trash.interval = 0</code> 默认关闭，其他值表示设置文件的存活时间，默认检查回收站的间隔时间是 <code>fs.trash.checkpoint.interval</code></li>
<li>快照管理，对目录做备份<ul>
<li><code>hdfs dfsadmin -allowSnapshot $dir</code> 开启指定目录的快照功能</li>
<li><code>hdfs dfsadmin -disallowSnapshot $dir</code> 关闭指定目录的快照功能</li>
<li><code>hdfs dfs -createSnapshot $dir [$name]</code> 创建指定名称的目录快照</li>
<li><code>hdfs dfs -renameSnapshot $dir $old_name $new_name </code>  重命名快照</li>
<li><code>hdfs lsSnapshottableDir</code> 列出当前目录所有可快照目录</li>
<li><code>hdfs snapshotDiff $dir1 $dir2</code> 比较两个快照目录</li>
<li><code>hdfs dfs -deleteSnapshot $dir $name</code> 删除快照</li>
</ul>
</li>
</ul>
<h2 id="4-NameNode-和-SecondaryNameNode"><a href="#4-NameNode-和-SecondaryNameNode" class="headerlink" title="4. NameNode 和 SecondaryNameNode"></a>4. NameNode 和 SecondaryNameNode</h2><h3 id="4-1-NN-和-2NN-的工作机制"><a href="#4-1-NN-和-2NN-的工作机制" class="headerlink" title="4.1 NN 和 2NN 的工作机制"></a>4.1 NN 和 2NN 的工作机制</h3><ul>
<li>HDFS 1.0<ul>
<li>元数据储存在内存中</li>
<li>磁盘中备份了元数据 FsImage</li>
<li>引入 Edits 文件，只进行追加操作。当元数据更新或添加时记录</li>
<li>2NN 用于定期完成 FsImage 和 Edits 的合并</li>
</ul>
</li>
<li>NN 的工作机制<ul>
<li>加载编辑日志和镜像文件到内存</li>
<li>客户端进行元数据到增删改</li>
<li>NN 记录操作日志，再更新滚动日志</li>
<li>进行内存数据的增删改</li>
</ul>
</li>
<li>2NN 的工作机制<ul>
<li>请求 NN 是否需要 CheckPoint</li>
<li>请求执行 CheckPoint</li>
<li>NN 滚动正在写的 Edits</li>
<li>拷贝 FsImage 和 Edits 到 2NN</li>
<li>合并，生成新的 FsImage 并拷贝到 NN</li>
</ul>
</li>
</ul>
<h3 id="4-2-FsImage-和-Edits"><a href="#4-2-FsImage-和-Edits" class="headerlink" title="4.2 FsImage 和 Edits"></a>4.2 FsImage 和 Edits</h3><ul>
<li><p>FsImage 和 Edits 在 <code>/data/tmp/dfs/name/current</code> 里</p>
</li>
<li><p><code>hdfs oiv</code> 和 <code>hdfs oev</code> 用于查看文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs oiv/oev -p XML -i <span class="variable">$input_file</span> -o <span class="variable">$output_file</span>.xml</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-3-CheckPoint-设置"><a href="#4-3-CheckPoint-设置" class="headerlink" title="4.3 CheckPoint 设置"></a>4.3 CheckPoint 设置</h3><ul>
<li><p>在 .xml 中有设置</p>
</li>
<li><p>默认 3600s</p>
</li>
<li><p>一分钟检查一次操作次数，达到 1000000 次立刻合并</p>
</li>
</ul>
<h3 id="4-4-NameNode-故障处理"><a href="#4-4-NameNode-故障处理" class="headerlink" title="4.4 NameNode 故障处理"></a>4.4 NameNode 故障处理</h3><ul>
<li><p>将 2NN 数据拷贝到 NN 储存数据到目录</p>
<ul>
<li>删除 <code>name</code> 文件夹</li>
<li>拷贝 <code>namesecondary</code> 文件夹到 <code>name</code> 文件夹</li>
</ul>
</li>
<li><p>使用 <code>-importCheckpoint</code> 选项启动 NN 守护进程，从而拷贝 2NN 到 NN</p>
<ul>
<li>第一步与上个方法一样</li>
<li>如果 2NN 不和 NN 在一个主机节点上，拷贝 2NN 储存数据的目录到 NN 储存数据的评级目录并删除 in_use.lock</li>
<li><code>hdfs namenode -importCheckpoint</code></li>
</ul>
</li>
</ul>
<h3 id="4-5-集群安全模式"><a href="#4-5-集群安全模式" class="headerlink" title="4.5 集群安全模式"></a>4.5 集群安全模式</h3><ul>
<li>NN 启动时首先将 FsImage 载入内存，并执行 Edits 中的各项操作</li>
<li>在内存中建立文件系统元数据的映像之后，会创建一个新的 FsImage 文件和一个空的 Edits， 此时 NN 开始监听 DataNode 的请求</li>
<li>以上两个过程中， NN 一直运行在安全模式中，即 NN 的文件系统对客户端是只读的</li>
<li>系统中数据块的位置以块列表形式储存在 DataNode 中，在系统正常操作期间 NN 会在内存中保存所有块位置的映射信息</li>
<li>在安全模式下， 各个 DataNode 会向 NN 发送最新的块列表信息</li>
<li>如果满足最小副本条件：在整个文件系统中 99.9% 的块满足最小副本级别 （默认值是 <code>dfs.relication.min=1</code>），则 NN 在 30s 后退出安全模式</li>
<li>集群处于安全模式下，不能执行重要操作（写操作）</li>
<li>安全模式基本命令<ul>
<li><code>hdfs dfsadmin -safemode get</code> 查看安全模式状态</li>
<li><code>hdfs dfsadmin -safemode enter</code> 进入安全模式</li>
<li><code>hdfs dfsadmin -safemode leave</code> 离开安全模式</li>
<li><code>hdfs dfsadmin -safemode wait</code> 等待安全模式状态</li>
</ul>
</li>
</ul>
<h2 id="5-DataNode"><a href="#5-DataNode" class="headerlink" title="5. DataNode"></a>5. DataNode</h2><h3 id="5-1-DataNode-工作机制"><a href="#5-1-DataNode-工作机制" class="headerlink" title="5.1 DataNode 工作机制"></a>5.1 DataNode 工作机制</h3><ul>
<li>DataNode 的 Block 中存放数据、数据长度、校验和、时间戳</li>
<li>DataNode 工作机制<ul>
<li>DN 启动后向 NN 注册</li>
<li>NN 将 DN 注册成功写入元数据</li>
<li>每个一个周期（一个小时） DN 上报所有块信息</li>
<li>心跳（每三秒一次）返回，结果带有 NN 给该 DN 的命令</li>
<li>超过十分钟没有收到 DN 的心跳包，则认为该节点不可用</li>
</ul>
</li>
</ul>
<h3 id="5-2-数据完整性"><a href="#5-2-数据完整性" class="headerlink" title="5.2 数据完整性"></a>5.2 数据完整性</h3><ul>
<li>保证数据完整性的方法<ul>
<li>当 DN 读取 Block 时，计算校验和</li>
<li>如果计算后的校验和与 Block 创建时不一样，说明 Block 已经损坏</li>
<li>Client 读取其他 DN 上的 Block</li>
<li>DN 在其文件创建后周期验证校验和</li>
</ul>
</li>
</ul>
<h3 id="5-3-掉线时限参数设置"><a href="#5-3-掉线时限参数设置" class="headerlink" title="5.3 掉线时限参数设置"></a>5.3 掉线时限参数设置</h3><ul>
<li>默认超时时长是 10min + 30s</li>
<li>计算公式为 $TimeOut = 2<em>dfs.namenode.heartbeat.recheckinterval + 10</em>dfs.heartbeat.interval$</li>
<li>配置在 hdfs-site.xml 文件中</li>
</ul>
<h3 id="5-4-添加新DN"><a href="#5-4-添加新DN" class="headerlink" title="5.4 添加新DN"></a>5.4 添加新DN</h3><ul>
<li>修改 IP 地址和 Host 文件</li>
<li>删除原来 HDFS 文件系统留存的文件 <code>hadoop-x.x.x/data</code> 和 <code>log</code></li>
<li>执行 <code>source /etc/profile</code></li>
</ul>
<h3 id="5-5-DN-退役旧数据节点"><a href="#5-5-DN-退役旧数据节点" class="headerlink" title="5.5 DN 退役旧数据节点"></a>5.5 DN 退役旧数据节点</h3><ul>
<li>不在白名单内的主机节点都会被退出<ul>
<li>白名单在 NN 的 <code>hadoop-x.x.x/etc/hadoop/dfs.hosts</code></li>
<li>在 hdfs-site.xml 配置文件中增加 dfs.hosts 属性</li>
<li>配置文件分发，刷新 NN</li>
<li>更新 ResourcesManager</li>
</ul>
</li>
<li>黑名单中的节点都会被踢出<ul>
<li>黑名单在 <code>hadoop-x-x-x/etc/hadoop/dfs.hosts.exclude</code></li>
<li>在黑名单中添加要退役节点的名称</li>
<li>在 hdfs-site.xml 配置文件中添加 dfs.hosts.exclude 属性</li>
<li>刷新 NN 和 RM</li>
</ul>
</li>
<li>黑名单和白名单中不能出现同一个主机名称</li>
</ul>
<h2 id="6-MapReduce"><a href="#6-MapReduce" class="headerlink" title="6. MapReduce"></a>6. MapReduce</h2><h3 id="6-1-MapReduce-概述"><a href="#6-1-MapReduce-概述" class="headerlink" title="6.1 MapReduce 概述"></a>6.1 MapReduce 概述</h3><ul>
<li>MapReduce 是分布式运算程序的编程框架</li>
<li>MapReduce 优点<ul>
<li>易于编程，实现一些接口即可完成一个分布式程序</li>
<li>良好的扩展性</li>
<li>高容错性，节点挂掉的时候计算任务可以自动转移</li>
<li>适合 <code>PB</code> 级别以上海量数据的离线处理</li>
</ul>
</li>
<li>MapReduce 缺点<ul>
<li>不擅长实时计算</li>
<li>不擅长流式计算</li>
<li>不擅长有向图计算</li>
</ul>
</li>
</ul>
<h3 id="6-2-MapReduce-核心思想"><a href="#6-2-MapReduce-核心思想" class="headerlink" title="6.2 MapReduce 核心思想"></a>6.2 MapReduce 核心思想</h3><blockquote>
<p>以单词统计为例</p>
</blockquote>
<ul>
<li>MapReduce 程序分为 Map 阶段和 Reduce 阶段</li>
<li>Map 阶段分发数据，启动 MapTask （并发）<ul>
<li>读取数据，并按行处理</li>
<li>按空格切分行内单词</li>
<li>KV 键值对按单词首字母，分为两个分区写入磁盘</li>
</ul>
</li>
<li>Reduce 阶段输入数据来源上个阶段的 MapTask输出<ul>
<li>启动 ReduceTask 从 MapTask 拉取相应数据</li>
<li>ReduceTask 输出各自数据</li>
</ul>
</li>
<li>MapReduce 模型只能包含一个 Map 阶段和一个 Reduce 阶段</li>
</ul>
<h3 id="6-3-MapReduce-进程"><a href="#6-3-MapReduce-进程" class="headerlink" title="6.3 MapReduce 进程"></a>6.3 MapReduce 进程</h3><ul>
<li>一个完整的 MapReduce 程序在分布式运行时有三类实例进程<ul>
<li>MrAppMaster 负责整个程序的过程调度及状态协调</li>
<li>MapTask 负责 Map 阶段的整个数据处理流程</li>
<li>ReduceTask 负责 Reduce 阶段的整个数据处理流程</li>
</ul>
</li>
</ul>
<h3 id="6-4-MapReduce-编程"><a href="#6-4-MapReduce-编程" class="headerlink" title="6.4 MapReduce 编程"></a>6.4 MapReduce 编程</h3><ul>
<li><p>常见类型的封装</p>
<table>
<thead>
<tr>
<th>Java 类型</th>
<th>Hadoop Writable 类型</th>
</tr>
</thead>
<tbody><tr>
<td>boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
</li>
<li><p>Mapper 阶段</p>
<ul>
<li>用户自定义的 Mapper 要继承父类</li>
<li>Mapper 的输入数据必须是键值对，但是键值的类型可以自己定义</li>
<li>Mapper 中的业务逻辑写在 <code>map()</code> 方法中</li>
<li>Mapper 的输出数据必须是键值对，但是键值的类型可以自己定义</li>
<li><code>map()</code> 方法（ MapTask 进程）对每个键值对调用一次</li>
</ul>
</li>
<li><p>Reduce 阶段</p>
<ul>
<li>用户自定义的 Reducer 要继承父类</li>
<li>Reducer 的输入数据对应 Mapper 的输出数据类型</li>
<li>Reducer 的业务逻辑写在 <code>reduce()</code> 方法中</li>
<li>ReduceTask 进程对每一组<strong>键相同</strong>的键值对调用一次 <code>reduce()</code> 方法</li>
</ul>
</li>
<li><p>Driver 阶段</p>
<ul>
<li>提交程序到 YARN 集群，提交的是封装了 MapReduce 程序相关运行参数的 <code>job</code> 对象</li>
</ul>
</li>
<li><p>Mapper <code>run()</code> 方法</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOExceptation, InterruptedExceptation</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(context.nextKeyValue())</span><br><span class="line">        &#123;</span><br><span class="line">            map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">finally</span></span><br><span class="line">    &#123;</span><br><span class="line">        cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Reducer <code>run()</code> 方法</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOExceptation, InterruptedExceptation</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(context.nextKey())</span><br><span class="line">        &#123;</span><br><span class="line">            reduce(context.getCurrentKey, context.getValues(), context);</span><br><span class="line">            Iterator&lt;VALUEIN&gt; iter = context.getValues().iterator();</span><br><span class="line">            <span class="keyword">if</span>(iter <span class="keyword">instanceof</span> ReduceContext.ValueIterator)</span><br><span class="line">            &#123;</span><br><span class="line">                ((ReduceContext.ValueIterator&lt;VALUEIN&gt;)iter).resetBackupStore();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">finally</span></span><br><span class="line">    &#123;</span><br><span class="line">        cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Driver 类内容<ul>
<li>获取 Job 对象</li>
<li>设置 .jar 储存位置</li>
<li>关联 Map 类和 Reduce 类</li>
<li>设置 Mapper 阶段输出数据的 Key 和 Value 类型</li>
<li>设置最终数据输出的 Key 和 Value 类型</li>
<li>设置输入路径和输出路径</li>
<li>提交 Job</li>
</ul>
</li>
</ul>
<h2 id="7-Hadoop-序列化"><a href="#7-Hadoop-序列化" class="headerlink" title="7. Hadoop 序列化"></a>7. Hadoop 序列化</h2><h3 id="7-1-序列化概述"><a href="#7-1-序列化概述" class="headerlink" title="7.1 序列化概述"></a>7.1 序列化概述</h3><ul>
<li>序列化就是把内存中的对象转换成字节序列或其他数据传输协议，以便于储存到磁盘和网络传输</li>
<li>反序列化就是将收到的字节序列或其他数据传输协议转换成内存中的对象</li>
<li>Java 的序列化自带很多额外信息，不便在网络中传输，因此 Hadoop开发了自己的序列化机制 Writable</li>
<li>Hadoop 序列化特点<ul>
<li>紧凑：高效使用储存空间</li>
<li>快速：数据读写额外开销小</li>
<li>可扩展：随着通信协议的升级而升级</li>
<li>互操作：支持多语言的交互</li>
</ul>
</li>
</ul>
<h3 id="7-2-Hadoop-序列化实现"><a href="#7-2-Hadoop-序列化实现" class="headerlink" title="7.2 Hadoop 序列化实现"></a>7.2 Hadoop 序列化实现</h3><ul>
<li>序列化对象必须实现 Writable 接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li>
<li>重写序列化方法 <code>write()</code> 和反序列化方法 <code>readFields()</code></li>
</ul>
<blockquote>
<p>以下是一个示例</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOExceptation</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    out.writeLong(upFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOExceptation</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    upFlow = in.readLong();</span><br><span class="line">    downFlow = in.readLong();</span><br><span class="line">    sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>反序列化的顺序必须和序列化的顺序<strong>完全一致</strong></li>
<li>如果要将结果显示在文件中，需要重写 <code>toString()</code> 方法</li>
<li>如果自定义反序列化的对象要在 Key 中进行传输，则还需要实现 Comparable 接口</li>
</ul>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><ul>
<li>下一节：<a href="./HadoopNote_02.md">MapReduce 框架原理</a></li>
</ul>

  </article>

  
      
    <div class="nexmoe-post-copyright">
        <strong>Author：</strong>John Doe<br>
        <strong>Link：</strong><a href="http://example.com/Notes/HadoopNote_01.html" title="http:&#x2F;&#x2F;example.com&#x2F;Notes&#x2F;HadoopNote_01.html" target="_blank" rel="noopener">http:&#x2F;&#x2F;example.com&#x2F;Notes&#x2F;HadoopNote_01.html</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
        
    </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
    
    
</div>

  
      <div class="nexmoe-post-footer">
          <section class="nexmoe-comment">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.css">
<div id="gitalk"></div>
<script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '80b2453b6d5f37ad6225',
        clientSecret: '43e99fa852795c9a7b3eb924b2558c64b84bbdeb',
        id: window.location.pathname,
        repo: 'nexmoe.github.io',
        owner: 'nexmoe',
        admin: 'nexmoe'
    })
    gitalk.render('gitalk')
</script>
</section>
      </div>
  
</div>
            <div class="nexmoe-post-right">
              <div class="nexmoe-fixed">
                  <div class="nexmoe-tool"> 
                    
                      
                    
                      <a href="#nexmoe-content" class="toc-link" aria-label="回到顶部" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
                  </div>
              </div>
            </div>
        </div>
    </div>
     
    <div id="nexmoe-search-space">
        <div class="search-container">
            <div class="search-header">
                <div class="search-input-container">
                    <input class="search-input" type="text" placeholder="Search" oninput="sinput();">
                </div>
                <a class="search-close" onclick="sclose();">×</a>
            </div>
            <div class="search-body"></div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/combine/npm/lazysizes@5.1.0/lazysizes.min.js,npm/mdui@0.4.3/dist/js/mdui.min.js?v=1"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

 

<script async src="/js/app.js?v=1630854088404"></script>



<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js"></script>
<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>


    





</body>

</html>
